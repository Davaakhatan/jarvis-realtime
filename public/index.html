<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Jarvis Voice Assistant</title>
  <style>
    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background: #0a0a0f;
      color: #e0e0e0;
      min-height: 100vh;
      display: flex;
      flex-direction: column;
      align-items: center;
      padding: 2rem;
    }
    h1 {
      color: #00d4ff;
      margin-bottom: 0.5rem;
    }
    .subtitle {
      color: #666;
      margin-bottom: 2rem;
    }
    .status {
      display: flex;
      align-items: center;
      gap: 0.5rem;
      margin-bottom: 1rem;
      padding: 0.5rem 1rem;
      background: #1a1a2e;
      border-radius: 20px;
    }
    .status-dot {
      width: 10px;
      height: 10px;
      border-radius: 50%;
      background: #ff4444;
    }
    .status-dot.connected {
      background: #44ff44;
    }
    .voice-button {
      width: 120px;
      height: 120px;
      border-radius: 50%;
      border: 3px solid #00d4ff;
      background: linear-gradient(145deg, #1a1a2e, #0a0a1a);
      color: #00d4ff;
      font-size: 2rem;
      cursor: pointer;
      transition: all 0.3s;
      display: flex;
      align-items: center;
      justify-content: center;
      margin: 2rem 0;
    }
    .voice-button:hover {
      transform: scale(1.05);
      box-shadow: 0 0 30px rgba(0, 212, 255, 0.3);
    }
    .voice-button.recording {
      background: linear-gradient(145deg, #2a1a2e, #1a0a1a);
      border-color: #ff4444;
      animation: pulse 1.5s infinite;
    }
    .voice-button.processing {
      border-color: #ffaa00;
    }
    .voice-button.speaking {
      border-color: #44ff44;
    }
    @keyframes pulse {
      0%, 100% { box-shadow: 0 0 20px rgba(255, 68, 68, 0.4); }
      50% { box-shadow: 0 0 40px rgba(255, 68, 68, 0.6); }
    }
    .state-label {
      font-size: 0.9rem;
      color: #888;
      margin-bottom: 2rem;
      text-transform: uppercase;
      letter-spacing: 2px;
    }
    .conversation {
      width: 100%;
      max-width: 600px;
      flex: 1;
      overflow-y: auto;
      padding: 1rem;
      background: #12121a;
      border-radius: 12px;
      margin-bottom: 1rem;
    }
    .message {
      padding: 1rem;
      margin-bottom: 1rem;
      border-radius: 12px;
      max-width: 85%;
    }
    .message.user {
      background: #1a2a3a;
      margin-left: auto;
      border-bottom-right-radius: 4px;
    }
    .message.assistant {
      background: #1a1a2e;
      border-bottom-left-radius: 4px;
    }
    .message .role {
      font-size: 0.75rem;
      color: #666;
      margin-bottom: 0.25rem;
      text-transform: uppercase;
    }
    .message.user .role {
      color: #00d4ff;
    }
    .message.assistant .role {
      color: #44ff44;
    }
    .interrupt-btn {
      position: fixed;
      bottom: 2rem;
      right: 2rem;
      padding: 1rem 2rem;
      background: #ff4444;
      color: white;
      border: none;
      border-radius: 8px;
      cursor: pointer;
      font-size: 1rem;
      display: none;
    }
    .interrupt-btn.visible {
      display: block;
    }
    .transcript {
      font-style: italic;
      color: #888;
      padding: 0.5rem;
      text-align: center;
    }
    .mode-toggle {
      display: flex;
      align-items: center;
      gap: 0.75rem;
      margin-bottom: 1rem;
      padding: 0.5rem 1rem;
      background: #1a1a2e;
      border-radius: 20px;
    }
    .toggle-switch {
      position: relative;
      width: 50px;
      height: 26px;
    }
    .toggle-switch input {
      opacity: 0;
      width: 0;
      height: 0;
    }
    .toggle-slider {
      position: absolute;
      cursor: pointer;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: #333;
      transition: 0.3s;
      border-radius: 26px;
    }
    .toggle-slider:before {
      position: absolute;
      content: "";
      height: 20px;
      width: 20px;
      left: 3px;
      bottom: 3px;
      background: white;
      transition: 0.3s;
      border-radius: 50%;
    }
    .toggle-switch input:checked + .toggle-slider {
      background: #00d4ff;
    }
    .toggle-switch input:checked + .toggle-slider:before {
      transform: translateX(24px);
    }
    .voice-button.listening-passive {
      border-color: #666;
      animation: pulse-passive 3s infinite;
    }
    @keyframes pulse-passive {
      0%, 100% { box-shadow: 0 0 10px rgba(102, 102, 102, 0.2); }
      50% { box-shadow: 0 0 20px rgba(102, 102, 102, 0.4); }
    }
    .wake-indicator {
      font-size: 0.8rem;
      color: #00d4ff;
      margin-top: -1rem;
      margin-bottom: 1rem;
      opacity: 0;
      transition: opacity 0.3s;
    }
    .wake-indicator.visible {
      opacity: 1;
    }
  </style>
</head>
<body>
  <h1>JARVIS</h1>
  <p class="subtitle">Real-time Voice Assistant</p>
  <p class="subtitle" style="font-size: 0.75rem; margin-top: -1rem;">Wake: "Jarvis" | Interrupt: "Stop", "Cancel", "Wait"</p>

  <div class="mode-toggle">
    <label class="toggle-switch">
      <input type="checkbox" id="alwaysListenToggle">
      <span class="toggle-slider"></span>
    </label>
    <span id="modeLabel">Push-to-talk</span>
  </div>

  <div class="status">
    <div class="status-dot" id="statusDot"></div>
    <span id="statusText">Disconnected</span>
  </div>

  <button class="voice-button" id="voiceBtn">üé§</button>
  <div class="wake-indicator" id="wakeIndicator">üéØ Wake word detected!</div>
  <div class="state-label" id="stateLabel">Press to speak</div>

  <div class="conversation" id="conversation"></div>

  <div class="transcript" id="transcript"></div>

  <button class="interrupt-btn" id="interruptBtn">‚èπ Interrupt</button>

  <script>
    const WS_URL = 'ws://localhost:3001';
    let ws = null;
    let mediaRecorder = null;
    let audioContext = null;
    let isRecording = false;
    let sessionId = null;
    let alwaysListening = false;
    let passiveStream = null;
    let passiveContext = null;
    let speechRecognition = null;
    let wakeWordTimeout = null;

    const voiceBtn = document.getElementById('voiceBtn');
    const stateLabel = document.getElementById('stateLabel');
    const statusDot = document.getElementById('statusDot');
    const statusText = document.getElementById('statusText');
    const conversation = document.getElementById('conversation');
    const transcript = document.getElementById('transcript');
    const interruptBtn = document.getElementById('interruptBtn');
    const alwaysListenToggle = document.getElementById('alwaysListenToggle');
    const modeLabel = document.getElementById('modeLabel');
    const wakeIndicator = document.getElementById('wakeIndicator');

    let reconnectAttempts = 0;
    const MAX_RECONNECT_DELAY = 30000;

    function connect() {
      ws = new WebSocket(WS_URL);

      ws.onopen = () => {
        reconnectAttempts = 0;
        statusDot.classList.add('connected');
        statusText.textContent = 'Connected';
        stateLabel.textContent = alwaysListening ? 'Say "Jarvis" to activate...' : 'Press to speak';

        // Resume always-listening if it was enabled
        if (alwaysListening) {
          startPassiveListening();
        }
      };

      ws.onclose = () => {
        statusDot.classList.remove('connected');
        statusText.textContent = 'Disconnected';

        // Stop any ongoing recording/playback
        cleanupRecording();
        stopPlayback();
        stopPassiveListening();

        // Exponential backoff with max delay
        reconnectAttempts++;
        const delay = Math.min(1000 * Math.pow(2, reconnectAttempts - 1), MAX_RECONNECT_DELAY);
        stateLabel.textContent = `Reconnecting in ${Math.round(delay/1000)}s...`;

        setTimeout(connect, delay);
      };

      ws.onerror = (error) => {
        console.error('WebSocket error:', error);
      };

      ws.onmessage = (event) => {
        const data = JSON.parse(event.data);
        handleMessage(data);
      };
    }

    function handleMessage(data) {
      console.log('Received:', data);

      switch (data.type) {
        case 'session.created':
          sessionId = data.sessionId;
          console.log('Session created:', sessionId);
          break;

        case 'transcript.partial':
          transcript.textContent = data.payload?.text || '';
          break;

        case 'transcript.final':
          transcript.textContent = '';
          addMessage('user', data.payload?.text || '');
          break;

        case 'llm.chunk':
          updateAssistantMessage(data.payload?.text || '');
          break;

        case 'llm.end':
          finalizeAssistantMessage();
          break;

        case 'tts.start':
          voiceBtn.className = 'voice-button speaking';
          stateLabel.textContent = 'Speaking... (speak to interrupt)';
          interruptBtn.classList.add('visible');
          audioChunksReceived = 0; // Reset chunk counter
          isTTSSpeaking = true;
          // Start listening for voice interrupt after a short delay
          // to avoid the user's last words from triggering interrupt
          setTimeout(() => {
            if (isTTSSpeaking) startInterruptVAD();
          }, 500);
          break;

        case 'tts.chunk':
          // Play audio chunk
          if (data.payload?.audio) {
            playAudio(data.payload.audio);
          }
          break;

        case 'tts.end':
          interruptBtn.classList.remove('visible');
          // Flush any remaining buffered audio
          if (audioQueue.length > 0 && !isPlaying) {
            playNextChunk();
          }
          // Wait for audio to finish before resuming listening
          const checkAudioDone = () => {
            if (audioQueue.length === 0 && !isPlaying) {
              isTTSSpeaking = false;
              stopInterruptVAD();
              if (alwaysListening) {
                startPassiveListening();
              } else {
                voiceBtn.className = 'voice-button';
                stateLabel.textContent = 'Press to speak';
              }
            } else {
              setTimeout(checkAudioDone, 100);
            }
          };
          checkAudioDone();
          break;

        case 'session.interrupt':
          interruptBtn.classList.remove('visible');
          stopPlayback(); // Stop any playing audio
          if (alwaysListening) {
            startPassiveListening();
          } else {
            voiceBtn.className = 'voice-button';
            stateLabel.textContent = 'Interrupted - Press to speak';
          }
          break;

        case 'session.state_change':
          console.log('State changed:', data.payload);
          break;

        case 'error':
          console.error('Error:', data.payload);
          stateLabel.textContent = 'Error: ' + (data.payload?.message || 'Unknown');
          break;
      }
    }

    let currentAssistantMessage = '';
    let assistantMessageEl = null;

    function addMessage(role, content) {
      const div = document.createElement('div');
      div.className = `message ${role}`;
      div.innerHTML = `<div class="role">${role}</div><div class="content">${content}</div>`;
      conversation.appendChild(div);
      conversation.scrollTop = conversation.scrollHeight;
      return div;
    }

    function updateAssistantMessage(chunk) {
      if (!assistantMessageEl) {
        assistantMessageEl = addMessage('assistant', '');
        currentAssistantMessage = '';
      }
      currentAssistantMessage += chunk;
      assistantMessageEl.querySelector('.content').textContent = currentAssistantMessage;
      conversation.scrollTop = conversation.scrollHeight;
    }

    function finalizeAssistantMessage() {
      assistantMessageEl = null;
      currentAssistantMessage = '';
    }

    let recordingAnalyser = null;
    let recordingStream = null;
    let recordingReady = false;
    let pendingStop = false;

    async function startRecording() {
      // Set isRecording immediately to prevent race conditions
      isRecording = true;
      recordingReady = false;
      pendingStop = false;
      voiceBtn.className = 'voice-button recording';
      stateLabel.textContent = 'Starting...';

      try {
        recordingStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            sampleRate: 16000,
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true
          }
        });

        // Check if stop was requested while waiting for mic permission
        if (pendingStop) {
          cleanupRecording();
          return;
        }

        audioContext = new AudioContext({ sampleRate: 16000 });
        const source = audioContext.createMediaStreamSource(recordingStream);
        const processor = audioContext.createScriptProcessor(4096, 1, 1);

        // Create analyser for silence detection
        recordingAnalyser = audioContext.createAnalyser();
        recordingAnalyser.fftSize = 512;
        recordingAnalyser.smoothingTimeConstant = 0.5;
        source.connect(recordingAnalyser);

        processor.onaudioprocess = (e) => {
          if (!isRecording || !recordingReady) return;

          const inputData = e.inputBuffer.getChannelData(0);
          const pcmData = new Int16Array(inputData.length);

          for (let i = 0; i < inputData.length; i++) {
            pcmData[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768));
          }

          if (ws && ws.readyState === WebSocket.OPEN) {
            ws.send(pcmData.buffer);
          }
        };

        source.connect(processor);
        processor.connect(audioContext.destination);

        recordingReady = true;
        stateLabel.textContent = 'Listening...';

        // Check again if stop was requested while setting up
        if (pendingStop) {
          stopRecording();
        }

      } catch (err) {
        console.error('Error accessing microphone:', err);
        isRecording = false;
        recordingReady = false;
        voiceBtn.className = 'voice-button';
        stateLabel.textContent = 'Microphone access denied';
      }
    }

    function cleanupRecording() {
      isRecording = false;
      recordingReady = false;
      recordingAnalyser = null;

      if (recordingStream) {
        recordingStream.getTracks().forEach(track => track.stop());
        recordingStream = null;
      }

      if (audioContext) {
        audioContext.close();
        audioContext = null;
      }

      voiceBtn.className = 'voice-button';
      stateLabel.textContent = 'Press to speak';
    }

    function stopRecording() {
      // If recording not ready yet, set flag to stop when ready
      if (!recordingReady && isRecording) {
        pendingStop = true;
        return;
      }

      const hadAudio = recordingReady;
      cleanupRecording();

      // Only show processing and send audio.end if we actually recorded something
      if (hadAudio) {
        voiceBtn.className = 'voice-button processing';
        stateLabel.textContent = 'Processing...';

        if (ws && ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify({ type: 'audio.end' }));
        }
      }

      // Don't restart passive listening here - wait for tts.end event
    }

    // Audio playback system for TTS
    let playbackContext = null;
    let audioQueue = [];
    let isPlaying = false;
    let audioChunksReceived = 0;
    let nextPlayTime = 0; // For seamless scheduling
    const TTS_SAMPLE_RATE = 24000; // OpenAI TTS default for PCM
    const MIN_BUFFER_CHUNKS = 3; // Buffer more chunks before starting playback

    // Voice interrupt during TTS playback
    let interruptVadContext = null;
    let interruptVadStream = null;
    let interruptVadAnalyser = null;
    let isTTSSpeaking = false;
    const INTERRUPT_THRESHOLD = 45; // Higher threshold to avoid TTS audio triggering it
    const INTERRUPT_CONSECUTIVE_FRAMES = 5; // Need sustained speech to interrupt
    let consecutiveSpeechFrames = 0;

    async function initPlaybackContext() {
      if (!playbackContext || playbackContext.state === 'closed') {
        playbackContext = new AudioContext({ sampleRate: TTS_SAMPLE_RATE });
      }
      if (playbackContext.state === 'suspended') {
        await playbackContext.resume();
      }
      return playbackContext;
    }

    function playAudio(audioData) {
      // audioData comes as { type: 'Buffer', data: [...] } from JSON-stringified Buffer
      if (!audioData || !audioData.data) {
        console.warn('Invalid audio data received');
        return;
      }

      // Convert byte array to Uint8Array, then create Int16Array view
      // This properly handles signed 16-bit PCM little-endian data
      const byteArray = new Uint8Array(audioData.data);
      const dataView = new DataView(byteArray.buffer);
      const int16Array = new Int16Array(byteArray.length / 2);

      for (let i = 0; i < int16Array.length; i++) {
        // getInt16 with littleEndian=true properly handles signed values
        int16Array[i] = dataView.getInt16(i * 2, true);
      }

      // Queue the audio chunk
      audioQueue.push(int16Array);
      audioChunksReceived++;

      // Log first chunk for debugging
      if (audioChunksReceived === 1) {
        console.log('First audio chunk received:', {
          byteLength: byteArray.length,
          samples: int16Array.length,
          duration: (int16Array.length / TTS_SAMPLE_RATE * 1000).toFixed(0) + 'ms',
          firstSamples: Array.from(int16Array.slice(0, 10))
        });
      }

      // Start playback after buffering enough chunks
      if (!isPlaying && audioQueue.length >= MIN_BUFFER_CHUNKS) {
        playNextChunk();
      }
    }

    async function playNextChunk() {
      if (audioQueue.length === 0) {
        isPlaying = false;
        nextPlayTime = 0;
        return;
      }

      isPlaying = true;
      const ctx = await initPlaybackContext();
      const int16Data = audioQueue.shift();

      // Convert Int16 to Float32 for Web Audio API
      const float32Data = new Float32Array(int16Data.length);
      for (let i = 0; i < int16Data.length; i++) {
        float32Data[i] = int16Data[i] / 32768;
      }

      // Create audio buffer
      const audioBuffer = ctx.createBuffer(1, float32Data.length, TTS_SAMPLE_RATE);
      audioBuffer.getChannelData(0).set(float32Data);

      // Create and play source directly to destination
      const source = ctx.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(ctx.destination);

      // Schedule seamlessly using precise timing
      const currentTime = ctx.currentTime;
      const startTime = nextPlayTime > currentTime ? nextPlayTime : currentTime;

      source.onended = () => {
        playNextChunk();
      };

      source.start(startTime);
      nextPlayTime = startTime + audioBuffer.duration;
    }

    function stopPlayback() {
      audioQueue = [];
      isPlaying = false;
      nextPlayTime = 0;
      isTTSSpeaking = false;
      stopInterruptVAD();
      if (playbackContext && playbackContext.state !== 'closed') {
        playbackContext.close();
        playbackContext = null;
      }
    }

    // Start listening for voice interrupt during TTS playback
    async function startInterruptVAD() {
      if (interruptVadStream) return; // Already running

      try {
        interruptVadStream = await navigator.mediaDevices.getUserMedia({
          audio: { echoCancellation: true, noiseSuppression: true }
        });

        interruptVadContext = new AudioContext();
        const source = interruptVadContext.createMediaStreamSource(interruptVadStream);
        interruptVadAnalyser = interruptVadContext.createAnalyser();
        interruptVadAnalyser.fftSize = 512;
        interruptVadAnalyser.smoothingTimeConstant = 0.3;
        source.connect(interruptVadAnalyser);

        consecutiveSpeechFrames = 0;
        monitorInterruptVAD();
        console.log('Voice interrupt VAD started');
      } catch (err) {
        console.warn('Could not start interrupt VAD:', err);
      }
    }

    function stopInterruptVAD() {
      if (interruptVadContext) {
        interruptVadContext.close();
        interruptVadContext = null;
      }
      if (interruptVadStream) {
        interruptVadStream.getTracks().forEach(track => track.stop());
        interruptVadStream = null;
      }
      interruptVadAnalyser = null;
      consecutiveSpeechFrames = 0;
    }

    function monitorInterruptVAD() {
      if (!interruptVadAnalyser || !isTTSSpeaking) return;

      const dataArray = new Uint8Array(interruptVadAnalyser.frequencyBinCount);
      interruptVadAnalyser.getByteFrequencyData(dataArray);
      const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;

      if (average > INTERRUPT_THRESHOLD) {
        consecutiveSpeechFrames++;
        if (consecutiveSpeechFrames >= INTERRUPT_CONSECUTIVE_FRAMES) {
          // User is speaking! Interrupt the assistant
          console.log('Voice interrupt detected! Stopping TTS...');
          onVoiceInterrupt();
          return;
        }
      } else {
        consecutiveSpeechFrames = 0;
      }

      if (isTTSSpeaking) {
        requestAnimationFrame(monitorInterruptVAD);
      }
    }

    async function onVoiceInterrupt() {
      // Stop TTS playback
      stopPlayback();

      // Send interrupt to server
      if (ws && ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify({ type: 'interrupt' }));
      }

      // Show feedback
      wakeIndicator.textContent = 'üõë Interrupted! Listening...';
      wakeIndicator.classList.add('visible');
      clearTimeout(wakeWordTimeout);
      wakeWordTimeout = setTimeout(() => {
        wakeIndicator.classList.remove('visible');
      }, 1500);

      // Start recording for the new question
      await startRecording();
      speechStartTime = Date.now();

      // Start silence detection for auto-stop (if in always-listening mode)
      if (alwaysListening) {
        startSilenceDetection();
      }
    }

    voiceBtn.addEventListener('mousedown', () => {
      if (!isRecording && ws && ws.readyState === WebSocket.OPEN) {
        startRecording();
      }
    });

    voiceBtn.addEventListener('mouseup', () => {
      if (isRecording) {
        stopRecording();
      }
    });

    voiceBtn.addEventListener('mouseleave', () => {
      if (isRecording) {
        stopRecording();
      }
    });

    interruptBtn.addEventListener('click', () => {
      stopPlayback(); // Stop audio immediately on client
      if (ws && ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify({ type: 'interrupt' }));
      }
    });

    // Keyboard shortcut: Space to talk
    document.addEventListener('keydown', (e) => {
      if (e.code === 'Space' && !e.repeat && !isRecording) {
        e.preventDefault();
        if (ws && ws.readyState === WebSocket.OPEN) {
          startRecording();
        }
      }
    });

    document.addEventListener('keyup', (e) => {
      if (e.code === 'Space' && isRecording) {
        e.preventDefault();
        stopRecording();
      }
    });

    // Wake Word Detection using Web Speech API
    let wakeWordRecognition = null;
    let speechRecognitionFailed = false; // Track if we should use VAD instead
    let vadContext = null;
    let vadStream = null;
    let vadAnalyser = null;
    let speechStartTime = null;
    let lastTriggerTime = 0;
    const WAKE_WORDS = ['jarvis', 'hey jarvis', 'okay jarvis', 'yo jarvis'];
    const SILENCE_THRESHOLD = 30;
    const SILENCE_DURATION = 1500;
    const MIN_SPEECH_DURATION = 500;
    const TRIGGER_COOLDOWN = 3000;

    function initWakeWordDetection() {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      if (!SpeechRecognition) {
        console.warn('Speech recognition not supported, falling back to VAD');
        return null;
      }

      const recognition = new SpeechRecognition();
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'en-US';

      recognition.onresult = (event) => {
        const last = event.results.length - 1;
        const transcript = event.results[last][0].transcript.toLowerCase().trim();

        // Check for wake word
        const hasWakeWord = WAKE_WORDS.some(wake => transcript.includes(wake));

        if (hasWakeWord && !isRecording) {
          // Extract command after wake word (if any)
          let command = transcript;
          for (const wake of WAKE_WORDS) {
            command = command.replace(wake, '').trim();
          }

          onWakeWordDetected(command);
        }
      };

      recognition.onerror = (event) => {
        if (event.error === 'network' || event.error === 'service-not-allowed' || event.error === 'not-allowed') {
          // Web Speech API not available, fall back to VAD permanently
          console.warn('Speech recognition unavailable, using VAD mode instead');
          speechRecognitionFailed = true;
          wakeWordRecognition = null;
          if (alwaysListening && !isRecording) {
            startVADListening();
          }
          return;
        }
        if (event.error !== 'no-speech' && event.error !== 'aborted') {
          console.warn('Speech recognition error:', event.error);
        }
        // Restart if still in always-listening mode
        if (alwaysListening && !isRecording && !speechRecognitionFailed) {
          setTimeout(() => {
            if (alwaysListening && !isRecording) {
              startWakeWordListening();
            }
          }, 500);
        }
      };

      recognition.onend = () => {
        // Restart if still in always-listening mode
        if (alwaysListening && !isRecording) {
          setTimeout(() => {
            if (alwaysListening && !isRecording) {
              startWakeWordListening();
            }
          }, 100);
        }
      };

      return recognition;
    }

    function startWakeWordListening() {
      // If speech recognition failed before, go straight to VAD
      if (speechRecognitionFailed) {
        startVADListening();
        return;
      }

      if (!wakeWordRecognition) {
        wakeWordRecognition = initWakeWordDetection();
      }

      if (wakeWordRecognition) {
        try {
          wakeWordRecognition.start();
          voiceBtn.className = 'voice-button listening-passive';
          stateLabel.textContent = 'Say "Jarvis" to activate...';
        } catch (e) {
          // Already started, ignore
        }
      } else {
        // Fallback to VAD
        startVADListening();
      }
    }

    function stopWakeWordListening() {
      if (wakeWordRecognition) {
        try {
          wakeWordRecognition.stop();
        } catch (e) {
          // Not started, ignore
        }
      }
      stopVADListening();
    }

    async function onWakeWordDetected(immediateCommand = '') {
      // Show indicator
      wakeIndicator.textContent = 'üéØ "Jarvis" detected!';
      wakeIndicator.classList.add('visible');
      clearTimeout(wakeWordTimeout);
      wakeWordTimeout = setTimeout(() => {
        wakeIndicator.classList.remove('visible');
      }, 2000);

      // Stop wake word listening
      stopWakeWordListening();

      // Start recording
      await startRecording();
      speechStartTime = Date.now();

      // Start silence detection for auto-stop
      startSilenceDetection();
    }

    // Fallback VAD for browsers without Speech Recognition
    function startVADListening() {
      stopVADListening();

      navigator.mediaDevices.getUserMedia({
        audio: { echoCancellation: true, noiseSuppression: true }
      }).then(stream => {
        vadStream = stream;
        vadContext = new AudioContext();
        const source = vadContext.createMediaStreamSource(stream);
        vadAnalyser = vadContext.createAnalyser();
        vadAnalyser.fftSize = 512;
        vadAnalyser.smoothingTimeConstant = 0.5;
        source.connect(vadAnalyser);

        voiceBtn.className = 'voice-button listening-passive';
        stateLabel.textContent = 'Listening... (speak to activate)';
        monitorAudioLevel();
      }).catch(err => {
        console.error('Failed to start VAD:', err);
        stateLabel.textContent = 'Microphone access denied';
        alwaysListenToggle.checked = false;
        alwaysListening = false;
        modeLabel.textContent = 'Push-to-talk';
      });
    }

    function stopVADListening() {
      if (vadContext) {
        vadContext.close();
        vadContext = null;
      }
      if (vadStream) {
        vadStream.getTracks().forEach(track => track.stop());
        vadStream = null;
      }
      vadAnalyser = null;
    }

    function monitorAudioLevel() {
      if (!vadAnalyser || !alwaysListening) return;

      const dataArray = new Uint8Array(vadAnalyser.frequencyBinCount);
      vadAnalyser.getByteFrequencyData(dataArray);
      const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;

      const now = Date.now();
      if (average > SILENCE_THRESHOLD && !isRecording && (now - lastTriggerTime) > TRIGGER_COOLDOWN) {
        lastTriggerTime = now;
        onVADSpeechDetected();
      }

      if (alwaysListening && !isRecording) {
        requestAnimationFrame(monitorAudioLevel);
      }
    }

    async function onVADSpeechDetected() {
      wakeIndicator.textContent = 'üéôÔ∏è Voice detected!';
      wakeIndicator.classList.add('visible');
      clearTimeout(wakeWordTimeout);
      wakeWordTimeout = setTimeout(() => {
        wakeIndicator.classList.remove('visible');
      }, 1500);

      stopVADListening();
      await startRecording();
      speechStartTime = Date.now();
      startSilenceDetection();
    }

    function startPassiveListening() {
      stopPassiveListening();
      startWakeWordListening();
    }

    function stopPassiveListening() {
      stopWakeWordListening();
      if (!isRecording) {
        voiceBtn.className = 'voice-button';
        stateLabel.textContent = 'Press to speak';
      }
    }

    function startSilenceDetection() {
      if (!recordingAnalyser || !isRecording || !recordingReady) return;

      let lastSpeechTime = Date.now();

      function checkSilence() {
        if (!isRecording || !alwaysListening || !recordingAnalyser || !recordingReady) return;

        const dataArray = new Uint8Array(recordingAnalyser.frequencyBinCount);
        recordingAnalyser.getByteFrequencyData(dataArray);
        const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;

        if (average > SILENCE_THRESHOLD) {
          lastSpeechTime = Date.now();
        }

        const silenceDuration = Date.now() - lastSpeechTime;
        const totalDuration = Date.now() - speechStartTime;

        // Stop if silence detected after minimum speech duration
        if (silenceDuration > SILENCE_DURATION && totalDuration > MIN_SPEECH_DURATION) {
          stopRecording();
          return;
        }

        // Max recording time of 30 seconds
        if (totalDuration > 30000) {
          stopRecording();
          return;
        }

        requestAnimationFrame(checkSilence);
      }

      // Start silence checking after a short delay
      setTimeout(checkSilence, 500);
    }

    // Toggle handler
    alwaysListenToggle.addEventListener('change', (e) => {
      alwaysListening = e.target.checked;
      modeLabel.textContent = alwaysListening ? 'Always listening' : 'Push-to-talk';

      if (alwaysListening) {
        startPassiveListening();
      } else {
        stopPassiveListening();
      }
    });

    // Connect on load
    connect();
  </script>
</body>
</html>
