<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Jarvis Voice Assistant</title>
  <style>
    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background: #0a0a0f;
      color: #e0e0e0;
      min-height: 100vh;
      display: flex;
      flex-direction: column;
      align-items: center;
      padding: 2rem;
    }
    h1 {
      color: #00d4ff;
      margin-bottom: 0.5rem;
    }
    .subtitle {
      color: #666;
      margin-bottom: 2rem;
    }
    .status {
      display: flex;
      align-items: center;
      gap: 0.5rem;
      margin-bottom: 1rem;
      padding: 0.5rem 1rem;
      background: #1a1a2e;
      border-radius: 20px;
    }
    .status-dot {
      width: 10px;
      height: 10px;
      border-radius: 50%;
      background: #ff4444;
    }
    .status-dot.connected {
      background: #44ff44;
    }
    .voice-button {
      width: 120px;
      height: 120px;
      border-radius: 50%;
      border: 3px solid #00d4ff;
      background: linear-gradient(145deg, #1a1a2e, #0a0a1a);
      color: #00d4ff;
      font-size: 2rem;
      cursor: pointer;
      transition: all 0.3s;
      display: flex;
      align-items: center;
      justify-content: center;
      margin: 2rem 0;
    }
    .voice-button:hover {
      transform: scale(1.05);
      box-shadow: 0 0 30px rgba(0, 212, 255, 0.3);
    }
    .voice-button.recording {
      background: linear-gradient(145deg, #2a1a2e, #1a0a1a);
      border-color: #ff4444;
      animation: pulse 1.5s infinite;
    }
    .voice-button.processing {
      border-color: #ffaa00;
    }
    .voice-button.speaking {
      border-color: #44ff44;
    }
    @keyframes pulse {
      0%, 100% { box-shadow: 0 0 20px rgba(255, 68, 68, 0.4); }
      50% { box-shadow: 0 0 40px rgba(255, 68, 68, 0.6); }
    }
    .state-label {
      font-size: 0.9rem;
      color: #888;
      margin-bottom: 2rem;
      text-transform: uppercase;
      letter-spacing: 2px;
      transition: color 0.3s, transform 0.2s;
    }
    .state-label.active {
      color: #00d4ff;
      transform: scale(1.05);
    }
    .state-label.error {
      color: #ff4444;
    }
    .conversation {
      width: 100%;
      max-width: 600px;
      flex: 1;
      overflow-y: auto;
      padding: 1rem;
      background: #12121a;
      border-radius: 12px;
      margin-bottom: 1rem;
    }
    .message {
      padding: 1rem;
      margin-bottom: 1rem;
      border-radius: 12px;
      max-width: 85%;
      position: relative;
      animation: slideIn 0.3s ease-out;
    }
    @keyframes slideIn {
      from {
        opacity: 0;
        transform: translateY(10px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }
    .message.user {
      background: #1a2a3a;
      margin-left: auto;
      border-bottom-right-radius: 4px;
    }
    .message.assistant {
      background: #1a1a2e;
      border-bottom-left-radius: 4px;
    }
    .message .role {
      font-size: 0.75rem;
      color: #666;
      margin-bottom: 0.25rem;
      text-transform: uppercase;
      display: flex;
      justify-content: space-between;
      align-items: center;
    }
    .message.user .role {
      color: #00d4ff;
    }
    .message.assistant .role {
      color: #44ff44;
    }
    .message .timestamp {
      font-size: 0.65rem;
      color: #555;
      font-weight: normal;
    }
    .conversation-header {
      display: flex;
      justify-content: space-between;
      align-items: center;
      padding: 0.75rem 1rem;
      background: #1a1a2e;
      border-radius: 12px 12px 0 0;
      margin-bottom: -1rem;
    }
    .conversation-title {
      font-size: 0.85rem;
      color: #888;
      text-transform: uppercase;
      letter-spacing: 1px;
    }
    .clear-history-btn {
      padding: 0.4rem 0.8rem;
      background: #ff4444;
      color: white;
      border: none;
      border-radius: 6px;
      cursor: pointer;
      font-size: 0.75rem;
      transition: background 0.2s;
    }
    .clear-history-btn:hover {
      background: #ff6666;
    }
    .clear-history-btn:active {
      background: #cc0000;
    }
    .empty-history {
      text-align: center;
      color: #555;
      padding: 2rem;
      font-style: italic;
    }
    .interrupt-btn {
      position: fixed;
      bottom: 2rem;
      right: 2rem;
      padding: 1rem 2rem;
      background: #ff4444;
      color: white;
      border: none;
      border-radius: 8px;
      cursor: pointer;
      font-size: 1rem;
      display: none;
    }
    .interrupt-btn.visible {
      display: block;
    }
    .transcript {
      font-style: italic;
      color: #888;
      padding: 0.5rem;
      text-align: center;
    }
    .mode-toggle {
      display: flex;
      align-items: center;
      gap: 0.75rem;
      margin-bottom: 1rem;
      padding: 0.5rem 1rem;
      background: #1a1a2e;
      border-radius: 20px;
    }
    .toggle-switch {
      position: relative;
      width: 50px;
      height: 26px;
    }
    .toggle-switch input {
      opacity: 0;
      width: 0;
      height: 0;
    }
    .toggle-slider {
      position: absolute;
      cursor: pointer;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: #333;
      transition: 0.3s;
      border-radius: 26px;
    }
    .toggle-slider:before {
      position: absolute;
      content: "";
      height: 20px;
      width: 20px;
      left: 3px;
      bottom: 3px;
      background: white;
      transition: 0.3s;
      border-radius: 50%;
    }
    .toggle-switch input:checked + .toggle-slider {
      background: #00d4ff;
    }
    .toggle-switch input:checked + .toggle-slider:before {
      transform: translateX(24px);
    }
    .voice-button.listening-passive {
      border-color: #666;
      animation: pulse-passive 3s infinite;
    }
    @keyframes pulse-passive {
      0%, 100% { box-shadow: 0 0 10px rgba(102, 102, 102, 0.2); }
      50% { box-shadow: 0 0 20px rgba(102, 102, 102, 0.4); }
    }
    .wake-indicator {
      font-size: 0.8rem;
      color: #00d4ff;
      margin-top: -1rem;
      margin-bottom: 1rem;
      opacity: 0;
      transition: opacity 0.3s, transform 0.3s;
      transform: translateY(-10px);
    }
    .wake-indicator.visible {
      opacity: 1;
      transform: translateY(0);
    }
    .wake-indicator.interrupt {
      color: #ff4444;
      animation: shake 0.5s;
    }
    @keyframes shake {
      0%, 100% { transform: translateX(0); }
      25% { transform: translateX(-10px); }
      75% { transform: translateX(10px); }
    }
    .activity-panel {
      display: flex;
      gap: 0.5rem;
      margin-bottom: 1rem;
      flex-wrap: wrap;
      justify-content: center;
    }
    .activity-indicator {
      padding: 0.4rem 0.8rem;
      background: #1a1a2e;
      border-radius: 12px;
      font-size: 0.75rem;
      color: #666;
      border: 1px solid transparent;
      transition: all 0.3s;
      opacity: 0.5;
    }
    .activity-indicator.active {
      opacity: 1;
      border-color: currentColor;
    }
    .activity-indicator.listening {
      color: #ff4444;
    }
    .activity-indicator.processing {
      color: #ffaa00;
    }
    .activity-indicator.speaking {
      color: #44ff44;
    }
    .activity-indicator.thinking {
      color: #00d4ff;
    }
    .error-banner {
      position: fixed;
      top: 2rem;
      left: 50%;
      transform: translateX(-50%) translateY(-150%);
      background: linear-gradient(145deg, #ff4444, #cc0000);
      color: white;
      padding: 1rem 2rem;
      border-radius: 12px;
      box-shadow: 0 4px 20px rgba(255, 68, 68, 0.5);
      z-index: 1000;
      transition: transform 0.4s;
      max-width: 80%;
      text-align: center;
    }
    .error-banner.visible {
      transform: translateX(-50%) translateY(0);
    }
    .error-banner .error-code {
      font-size: 0.7rem;
      opacity: 0.8;
      margin-top: 0.25rem;
    }
  </style>
</head>
<body>
  <h1>JARVIS</h1>
  <p class="subtitle">Real-time Voice Assistant</p>
  <p class="subtitle" style="font-size: 0.75rem; margin-top: -1rem;">Wake: "Jarvis" | Interrupt: "Stop", "Cancel", "Wait"</p>

  <div class="mode-toggle">
    <label class="toggle-switch">
      <input type="checkbox" id="alwaysListenToggle">
      <span class="toggle-slider"></span>
    </label>
    <span id="modeLabel">Push-to-talk</span>
  </div>

  <div class="status">
    <div class="status-dot" id="statusDot"></div>
    <span id="statusText">Disconnected</span>
  </div>

  <div class="activity-panel" id="activityPanel">
    <div class="activity-indicator listening" id="activityListening">üé§ Listening</div>
    <div class="activity-indicator thinking" id="activityThinking">üí≠ Thinking</div>
    <div class="activity-indicator speaking" id="activitySpeaking">üîä Speaking</div>
  </div>

  <button class="voice-button" id="voiceBtn">üé§</button>
  <div class="wake-indicator" id="wakeIndicator">üéØ Wake word detected!</div>
  <div class="state-label" id="stateLabel">Press to speak</div>

  <div class="error-banner" id="errorBanner">
    <div class="error-message"></div>
    <div class="error-code"></div>
  </div>

  <div class="conversation-header">
    <div class="conversation-title">Conversation History</div>
    <button class="clear-history-btn" id="clearHistoryBtn">Clear History</button>
  </div>
  <div class="conversation" id="conversation">
    <div class="empty-history">No messages yet. Start a conversation!</div>
  </div>

  <div class="transcript" id="transcript"></div>

  <button class="interrupt-btn" id="interruptBtn">‚èπ Interrupt</button>

  <script>
    const WS_URL = 'ws://localhost:3001';
    let ws = null;
    let mediaRecorder = null;
    let audioContext = null;
    let isRecording = false;
    let sessionId = null;
    let alwaysListening = false;
    let passiveStream = null;
    let passiveContext = null;
    let speechRecognition = null;
    let wakeWordTimeout = null;

    const voiceBtn = document.getElementById('voiceBtn');
    const stateLabel = document.getElementById('stateLabel');
    const statusDot = document.getElementById('statusDot');
    const statusText = document.getElementById('statusText');
    const conversation = document.getElementById('conversation');
    const transcript = document.getElementById('transcript');
    const interruptBtn = document.getElementById('interruptBtn');
    const alwaysListenToggle = document.getElementById('alwaysListenToggle');
    const modeLabel = document.getElementById('modeLabel');
    const wakeIndicator = document.getElementById('wakeIndicator');
    const activityListening = document.getElementById('activityListening');
    const activityThinking = document.getElementById('activityThinking');
    const activitySpeaking = document.getElementById('activitySpeaking');
    const errorBanner = document.getElementById('errorBanner');
    const clearHistoryBtn = document.getElementById('clearHistoryBtn');

    let reconnectAttempts = 0;
    const MAX_RECONNECT_DELAY = 30000;
    let errorBannerTimeout = null;

    // Conversation history management
    const HISTORY_KEY = 'jarvis_conversation_history';
    let conversationHistory = [];

    // Load history from localStorage
    function loadHistory() {
      try {
        const saved = localStorage.getItem(HISTORY_KEY);
        if (saved) {
          conversationHistory = JSON.parse(saved);
          renderHistory();
        }
      } catch (err) {
        console.warn('Failed to load conversation history:', err);
      }
    }

    // Save history to localStorage
    function saveHistory() {
      try {
        // Keep only last 100 messages to avoid localStorage limits
        const recentHistory = conversationHistory.slice(-100);
        localStorage.setItem(HISTORY_KEY, JSON.stringify(recentHistory));
      } catch (err) {
        console.warn('Failed to save conversation history:', err);
      }
    }

    // Render entire history
    function renderHistory() {
      conversation.innerHTML = '';
      if (conversationHistory.length === 0) {
        conversation.innerHTML = '<div class="empty-history">No messages yet. Start a conversation!</div>';
        return;
      }
      conversationHistory.forEach(msg => {
        const div = createMessageElement(msg.role, msg.content, msg.timestamp);
        conversation.appendChild(div);
      });
      conversation.scrollTop = conversation.scrollHeight;
    }

    // Create message element
    function createMessageElement(role, content, timestamp) {
      const div = document.createElement('div');
      div.className = `message ${role}`;
      const time = new Date(timestamp).toLocaleTimeString('en-US', {
        hour: '2-digit',
        minute: '2-digit'
      });
      div.innerHTML = `
        <div class="role">
          ${role}
          <span class="timestamp">${time}</span>
        </div>
        <div class="content">${content}</div>
      `;
      return div;
    }

    // Clear history
    function clearHistory() {
      if (conversationHistory.length === 0) return;

      if (confirm('Clear entire conversation history?')) {
        conversationHistory = [];
        saveHistory();
        renderHistory();
      }
    }

    // Clear history button handler
    clearHistoryBtn.addEventListener('click', clearHistory);

    // Activity indicator helpers
    function setActivityState(listening = false, thinking = false, speaking = false) {
      activityListening.classList.toggle('active', listening);
      activityThinking.classList.toggle('active', thinking);
      activitySpeaking.classList.toggle('active', speaking);
    }

    // Error display helper
    function showError(message, code = '') {
      const banner = errorBanner;
      banner.querySelector('.error-message').textContent = message;
      banner.querySelector('.error-code').textContent = code ? `Error: ${code}` : '';
      banner.classList.add('visible');

      clearTimeout(errorBannerTimeout);
      errorBannerTimeout = setTimeout(() => {
        banner.classList.remove('visible');
      }, 5000);
    }

    function connect() {
      ws = new WebSocket(WS_URL);

      ws.onopen = () => {
        reconnectAttempts = 0;
        statusDot.classList.add('connected');
        statusText.textContent = 'Connected';
        stateLabel.textContent = alwaysListening ? 'Say "Jarvis" to activate...' : 'Press to speak';

        // Resume always-listening if it was enabled
        if (alwaysListening) {
          startPassiveListening();
        }
      };

      ws.onclose = () => {
        statusDot.classList.remove('connected');
        statusText.textContent = 'Disconnected';

        // Stop any ongoing recording/playback
        cleanupRecording();
        stopPlayback();
        stopPassiveListening();

        // Exponential backoff with max delay
        reconnectAttempts++;
        const delay = Math.min(1000 * Math.pow(2, reconnectAttempts - 1), MAX_RECONNECT_DELAY);
        stateLabel.textContent = `Reconnecting in ${Math.round(delay/1000)}s...`;

        setTimeout(connect, delay);
      };

      ws.onerror = (error) => {
        console.error('WebSocket error:', error);
      };

      ws.onmessage = (event) => {
        const data = JSON.parse(event.data);
        handleMessage(data);
      };
    }

    function handleMessage(data) {
      console.log('Received:', data);

      switch (data.type) {
        case 'session.created':
          sessionId = data.sessionId;
          console.log('Session created:', sessionId);
          break;

        case 'transcript.partial':
          transcript.textContent = data.payload?.text || '';
          break;

        case 'transcript.final':
          transcript.textContent = '';
          addMessage('user', data.payload?.text || '');
          setActivityState(false, true, false);
          stateLabel.textContent = 'Thinking...';
          stateLabel.classList.add('active');
          break;

        case 'llm.start':
          setActivityState(false, true, false);
          stateLabel.textContent = 'Generating response...';
          stateLabel.classList.add('active');
          break;

        case 'llm.chunk':
          updateAssistantMessage(data.payload?.text || '');
          break;

        case 'llm.end':
          finalizeAssistantMessage();
          setActivityState(false, false, false);
          stateLabel.classList.remove('active');
          break;

        case 'tts.start':
          voiceBtn.className = 'voice-button speaking';
          stateLabel.textContent = 'Speaking... (speak to interrupt)';
          stateLabel.classList.add('active');
          setActivityState(false, false, true);
          interruptBtn.classList.add('visible');
          audioChunksReceived = 0; // Reset chunk counter
          isTTSSpeaking = true;
          // Start listening for voice interrupt after a short delay
          // to avoid the user's last words from triggering interrupt
          setTimeout(() => {
            if (isTTSSpeaking) startInterruptVAD();
          }, 500);
          break;

        case 'tts.chunk':
          // Play audio chunk
          if (data.payload?.audio) {
            playAudio(data.payload.audio);
          }
          break;

        case 'tts.stop':
          // Immediately stop TTS playback (before interrupt)
          stopPlayback();
          interruptBtn.classList.remove('visible');
          setActivityState(false, false, false);
          stateLabel.classList.remove('active');
          // Show interrupt feedback
          wakeIndicator.textContent = '‚èπ Audio stopped';
          wakeIndicator.classList.add('visible', 'interrupt');
          clearTimeout(wakeWordTimeout);
          wakeWordTimeout = setTimeout(() => {
            wakeIndicator.classList.remove('visible', 'interrupt');
          }, 1500);
          break;

        case 'tts.end':
          interruptBtn.classList.remove('visible');
          stateLabel.classList.remove('active');
          setActivityState(false, false, false);
          // Wait for audio to finish before resuming listening
          const checkAudioDone = () => {
            if (!isPlaying) {
              isTTSSpeaking = false;
              stopInterruptVAD();
              if (alwaysListening) {
                startPassiveListening();
              } else {
                voiceBtn.className = 'voice-button';
                stateLabel.textContent = 'Press to speak';
              }
            } else {
              setTimeout(checkAudioDone, 100);
            }
          };
          checkAudioDone();
          break;

        case 'session.interrupt':
          interruptBtn.classList.remove('visible');
          stateLabel.classList.remove('active');
          setActivityState(false, false, false);
          stopPlayback(); // Stop any playing audio
          if (alwaysListening) {
            startPassiveListening();
          } else {
            voiceBtn.className = 'voice-button';
            stateLabel.textContent = 'Interrupted - Press to speak';
          }
          break;

        case 'session.state_change':
          console.log('State changed:', data.payload);
          break;

        case 'error':
          console.error('Error:', data.payload);
          const errorMsg = data.payload?.message || 'Unknown error occurred';
          const errorCode = data.payload?.code || '';
          stateLabel.textContent = 'Error occurred';
          stateLabel.classList.add('error');
          setActivityState(false, false, false);
          showError(errorMsg, errorCode);
          setTimeout(() => {
            stateLabel.classList.remove('error');
            if (alwaysListening) {
              startPassiveListening();
            } else {
              stateLabel.textContent = 'Press to speak';
            }
          }, 3000);
          break;
      }
    }

    let currentAssistantMessage = '';
    let assistantMessageEl = null;
    let currentAssistantTimestamp = null;

    function addMessage(role, content) {
      const timestamp = Date.now();

      // Remove empty history placeholder if present
      const emptyMsg = conversation.querySelector('.empty-history');
      if (emptyMsg) {
        emptyMsg.remove();
      }

      // Add to history
      conversationHistory.push({ role, content, timestamp });
      saveHistory();

      // Create and append message element
      const div = createMessageElement(role, content, timestamp);
      conversation.appendChild(div);
      conversation.scrollTop = conversation.scrollHeight;
      return div;
    }

    function updateAssistantMessage(chunk) {
      if (!assistantMessageEl) {
        // Remove empty history placeholder if present
        const emptyMsg = conversation.querySelector('.empty-history');
        if (emptyMsg) {
          emptyMsg.remove();
        }

        currentAssistantMessage = '';
        currentAssistantTimestamp = Date.now();

        // Create element but don't add to history yet (it's still streaming)
        assistantMessageEl = createMessageElement('assistant', '', currentAssistantTimestamp);
        conversation.appendChild(assistantMessageEl);
      }

      currentAssistantMessage += chunk;
      assistantMessageEl.querySelector('.content').textContent = currentAssistantMessage;
      conversation.scrollTop = conversation.scrollHeight;
    }

    function finalizeAssistantMessage() {
      if (assistantMessageEl && currentAssistantMessage) {
        // Now add the complete message to history
        conversationHistory.push({
          role: 'assistant',
          content: currentAssistantMessage,
          timestamp: currentAssistantTimestamp
        });
        saveHistory();
      }

      assistantMessageEl = null;
      currentAssistantMessage = '';
      currentAssistantTimestamp = null;
    }

    let recordingAnalyser = null;
    let recordingStream = null;
    let recordingReady = false;
    let pendingStop = false;

    async function startRecording() {
      // Set isRecording immediately to prevent race conditions
      isRecording = true;
      recordingReady = false;
      pendingStop = false;
      voiceBtn.className = 'voice-button recording';
      stateLabel.textContent = 'Starting...';
      stateLabel.classList.add('active');
      setActivityState(true, false, false);

      try {
        recordingStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            sampleRate: 16000,
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true
          }
        });

        // Check if stop was requested while waiting for mic permission
        if (pendingStop) {
          cleanupRecording();
          return;
        }

        audioContext = new AudioContext({ sampleRate: 16000 });
        const source = audioContext.createMediaStreamSource(recordingStream);
        const processor = audioContext.createScriptProcessor(4096, 1, 1);

        // Create analyser for silence detection
        recordingAnalyser = audioContext.createAnalyser();
        recordingAnalyser.fftSize = 512;
        recordingAnalyser.smoothingTimeConstant = 0.5;
        source.connect(recordingAnalyser);

        processor.onaudioprocess = (e) => {
          if (!isRecording || !recordingReady) return;

          const inputData = e.inputBuffer.getChannelData(0);
          const pcmData = new Int16Array(inputData.length);

          for (let i = 0; i < inputData.length; i++) {
            pcmData[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768));
          }

          if (ws && ws.readyState === WebSocket.OPEN) {
            ws.send(pcmData.buffer);
          }
        };

        source.connect(processor);
        processor.connect(audioContext.destination);

        recordingReady = true;
        stateLabel.textContent = 'Listening...';
        stateLabel.classList.add('active');
        setActivityState(true, false, false);

        // Check again if stop was requested while setting up
        if (pendingStop) {
          stopRecording();
        }

      } catch (err) {
        console.error('Error accessing microphone:', err);
        isRecording = false;
        recordingReady = false;
        voiceBtn.className = 'voice-button';
        stateLabel.textContent = 'Microphone access denied';
        stateLabel.classList.add('error');
        setActivityState(false, false, false);
        showError('Microphone access denied. Please allow microphone access.', 'MIC_DENIED');
        setTimeout(() => {
          stateLabel.classList.remove('error');
          stateLabel.textContent = 'Press to speak';
        }, 3000);
      }
    }

    function cleanupRecording() {
      isRecording = false;
      recordingReady = false;
      recordingAnalyser = null;

      if (recordingStream) {
        recordingStream.getTracks().forEach(track => track.stop());
        recordingStream = null;
      }

      if (audioContext) {
        audioContext.close();
        audioContext = null;
      }

      voiceBtn.className = 'voice-button';
      stateLabel.textContent = 'Press to speak';
      stateLabel.classList.remove('active');
      setActivityState(false, false, false);
    }

    function stopRecording() {
      // If recording not ready yet, set flag to stop when ready
      if (!recordingReady && isRecording) {
        pendingStop = true;
        return;
      }

      const hadAudio = recordingReady;
      cleanupRecording();

      // Only show processing and send audio.end if we actually recorded something
      if (hadAudio) {
        voiceBtn.className = 'voice-button processing';
        stateLabel.textContent = 'Processing...';
        stateLabel.classList.add('active');
        setActivityState(false, true, false);

        if (ws && ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify({ type: 'audio.end' }));
        }
      }

      // Don't restart passive listening here - wait for tts.end event
    }

    // Audio playback system for TTS (MP3 format)
    let playbackContext = null;
    let isPlaying = false;
    let audioChunksReceived = 0;
    let currentSource = null; // Track current playing source for interruption

    // Voice interrupt during TTS playback
    let interruptVadContext = null;
    let interruptVadStream = null;
    let interruptVadAnalyser = null;
    let isTTSSpeaking = false;
    const INTERRUPT_THRESHOLD = 45; // Higher threshold to avoid TTS audio triggering it
    const INTERRUPT_CONSECUTIVE_FRAMES = 5; // Need sustained speech to interrupt
    let consecutiveSpeechFrames = 0;

    async function initPlaybackContext() {
      if (!playbackContext || playbackContext.state === 'closed') {
        playbackContext = new AudioContext();
      }
      if (playbackContext.state === 'suspended') {
        await playbackContext.resume();
      }
      return playbackContext;
    }

    async function playAudio(audioData) {
      // Block audio if interrupted
      if (isInterrupted) {
        console.log('Audio blocked - currently interrupted');
        return;
      }

      // audioData comes as { type: 'Buffer', data: [...] } from JSON-stringified Buffer
      if (!audioData || !audioData.data) {
        console.warn('Invalid audio data received');
        return;
      }

      audioChunksReceived++;
      const byteArray = new Uint8Array(audioData.data);

      console.log('MP3 audio received:', {
        byteLength: byteArray.length,
        chunkNumber: audioChunksReceived
      });

      try {
        const ctx = await initPlaybackContext();

        // Check again after await in case interrupted during context init
        if (isInterrupted) {
          console.log('Audio blocked after context init - interrupted');
          return;
        }

        // Decode MP3 using browser's built-in decoder - produces clean audio
        const audioBuffer = await ctx.decodeAudioData(byteArray.buffer.slice(0));

        // Check again after decode in case interrupted during decoding
        if (isInterrupted) {
          console.log('Audio blocked after decode - interrupted');
          return;
        }

        console.log('Audio decoded:', {
          duration: audioBuffer.duration.toFixed(2) + 's',
          sampleRate: audioBuffer.sampleRate,
          channels: audioBuffer.numberOfChannels
        });

        // Create source and play
        const source = ctx.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(ctx.destination);

        currentSource = source;
        isPlaying = true;

        source.onended = () => {
          isPlaying = false;
          currentSource = null;
        };

        source.start(0);
      } catch (err) {
        console.error('Failed to decode/play audio:', err);
      }
    }

    let isInterrupted = false; // Flag to block new audio during interrupt

    function stopPlayback() {
      console.log('stopPlayback called - stopping audio immediately');
      isInterrupted = true; // Block any new audio chunks
      isPlaying = false;
      isTTSSpeaking = false;
      stopInterruptVAD();

      // First stop the source node - this immediately stops audio
      if (currentSource) {
        try {
          currentSource.disconnect(); // Disconnect from destination first
          currentSource.stop(0); // Stop immediately
          console.log('Audio source stopped');
        } catch (e) {
          console.log('Source already stopped or error:', e.message);
        }
        currentSource = null;
      }

      // Then close the context
      if (playbackContext && playbackContext.state !== 'closed') {
        try {
          playbackContext.close();
          console.log('Playback context closed');
        } catch (e) {
          console.log('Context close error:', e.message);
        }
        playbackContext = null;
      }

      // Reset interrupt flag after a short delay to allow new audio
      setTimeout(() => {
        isInterrupted = false;
      }, 100);
    }

    // Start listening for voice interrupt during TTS playback
    async function startInterruptVAD() {
      if (interruptVadStream) return; // Already running

      try {
        interruptVadStream = await navigator.mediaDevices.getUserMedia({
          audio: { echoCancellation: true, noiseSuppression: true }
        });

        interruptVadContext = new AudioContext();
        const source = interruptVadContext.createMediaStreamSource(interruptVadStream);
        interruptVadAnalyser = interruptVadContext.createAnalyser();
        interruptVadAnalyser.fftSize = 512;
        interruptVadAnalyser.smoothingTimeConstant = 0.3;
        source.connect(interruptVadAnalyser);

        consecutiveSpeechFrames = 0;
        monitorInterruptVAD();
        console.log('Voice interrupt VAD started');
      } catch (err) {
        console.warn('Could not start interrupt VAD:', err);
      }
    }

    function stopInterruptVAD() {
      if (interruptVadContext) {
        interruptVadContext.close();
        interruptVadContext = null;
      }
      if (interruptVadStream) {
        interruptVadStream.getTracks().forEach(track => track.stop());
        interruptVadStream = null;
      }
      interruptVadAnalyser = null;
      consecutiveSpeechFrames = 0;
    }

    function monitorInterruptVAD() {
      if (!interruptVadAnalyser || !isTTSSpeaking) return;

      const dataArray = new Uint8Array(interruptVadAnalyser.frequencyBinCount);
      interruptVadAnalyser.getByteFrequencyData(dataArray);
      const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;

      if (average > INTERRUPT_THRESHOLD) {
        consecutiveSpeechFrames++;
        if (consecutiveSpeechFrames >= INTERRUPT_CONSECUTIVE_FRAMES) {
          // User is speaking! Interrupt the assistant
          console.log('Voice interrupt detected! Stopping TTS...');
          onVoiceInterrupt();
          return;
        }
      } else {
        consecutiveSpeechFrames = 0;
      }

      if (isTTSSpeaking) {
        requestAnimationFrame(monitorInterruptVAD);
      }
    }

    async function onVoiceInterrupt() {
      // Stop TTS playback
      stopPlayback();

      // Send interrupt to server
      if (ws && ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify({ type: 'interrupt' }));
      }

      // Show feedback
      wakeIndicator.textContent = 'üõë Interrupted! Listening...';
      wakeIndicator.classList.add('visible', 'interrupt');
      clearTimeout(wakeWordTimeout);
      wakeWordTimeout = setTimeout(() => {
        wakeIndicator.classList.remove('visible', 'interrupt');
      }, 1500);

      // Start recording for the new question
      await startRecording();
      speechStartTime = Date.now();

      // Start silence detection for auto-stop (if in always-listening mode)
      if (alwaysListening) {
        startSilenceDetection();
      }
    }

    voiceBtn.addEventListener('mousedown', () => {
      if (!isRecording && ws && ws.readyState === WebSocket.OPEN) {
        startRecording();
      }
    });

    voiceBtn.addEventListener('mouseup', () => {
      if (isRecording) {
        stopRecording();
      }
    });

    voiceBtn.addEventListener('mouseleave', () => {
      if (isRecording) {
        stopRecording();
      }
    });

    interruptBtn.addEventListener('click', () => {
      stopPlayback(); // Stop audio immediately on client
      if (ws && ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify({ type: 'interrupt' }));
      }
    });

    // Keyboard shortcut: Space to talk
    document.addEventListener('keydown', (e) => {
      if (e.code === 'Space' && !e.repeat && !isRecording) {
        e.preventDefault();
        if (ws && ws.readyState === WebSocket.OPEN) {
          startRecording();
        }
      }
    });

    document.addEventListener('keyup', (e) => {
      if (e.code === 'Space' && isRecording) {
        e.preventDefault();
        stopRecording();
      }
    });

    // Wake Word Detection using Web Speech API
    let wakeWordRecognition = null;
    let speechRecognitionFailed = false; // Track if we should use VAD instead
    let vadContext = null;
    let vadStream = null;
    let vadAnalyser = null;
    let speechStartTime = null;
    let lastTriggerTime = 0;
    const WAKE_WORDS = ['jarvis', 'hey jarvis', 'okay jarvis', 'yo jarvis'];
    const SILENCE_THRESHOLD = 30;
    const SILENCE_DURATION = 1500;
    const MIN_SPEECH_DURATION = 500;
    const TRIGGER_COOLDOWN = 3000;

    function initWakeWordDetection() {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      if (!SpeechRecognition) {
        console.warn('Speech recognition not supported, falling back to VAD');
        return null;
      }

      const recognition = new SpeechRecognition();
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'en-US';

      recognition.onresult = (event) => {
        const last = event.results.length - 1;
        const transcript = event.results[last][0].transcript.toLowerCase().trim();

        // Check for wake word
        const hasWakeWord = WAKE_WORDS.some(wake => transcript.includes(wake));

        if (hasWakeWord && !isRecording) {
          // Extract command after wake word (if any)
          let command = transcript;
          for (const wake of WAKE_WORDS) {
            command = command.replace(wake, '').trim();
          }

          onWakeWordDetected(command);
        }
      };

      recognition.onerror = (event) => {
        if (event.error === 'network' || event.error === 'service-not-allowed' || event.error === 'not-allowed') {
          // Web Speech API not available, fall back to VAD permanently
          console.warn('Speech recognition unavailable, using VAD mode instead');
          speechRecognitionFailed = true;
          wakeWordRecognition = null;
          if (alwaysListening && !isRecording) {
            startVADListening();
          }
          return;
        }
        if (event.error !== 'no-speech' && event.error !== 'aborted') {
          console.warn('Speech recognition error:', event.error);
        }
        // Restart if still in always-listening mode
        if (alwaysListening && !isRecording && !speechRecognitionFailed) {
          setTimeout(() => {
            if (alwaysListening && !isRecording) {
              startWakeWordListening();
            }
          }, 500);
        }
      };

      recognition.onend = () => {
        // Restart if still in always-listening mode
        if (alwaysListening && !isRecording) {
          setTimeout(() => {
            if (alwaysListening && !isRecording) {
              startWakeWordListening();
            }
          }, 100);
        }
      };

      return recognition;
    }

    function startWakeWordListening() {
      // If speech recognition failed before, go straight to VAD
      if (speechRecognitionFailed) {
        startVADListening();
        return;
      }

      if (!wakeWordRecognition) {
        wakeWordRecognition = initWakeWordDetection();
      }

      if (wakeWordRecognition) {
        try {
          wakeWordRecognition.start();
          voiceBtn.className = 'voice-button listening-passive';
          stateLabel.textContent = 'Say "Jarvis" to activate...';
        } catch (e) {
          // Already started, ignore
        }
      } else {
        // Fallback to VAD
        startVADListening();
      }
    }

    function stopWakeWordListening() {
      if (wakeWordRecognition) {
        try {
          wakeWordRecognition.stop();
        } catch (e) {
          // Not started, ignore
        }
      }
      stopVADListening();
    }

    async function onWakeWordDetected(immediateCommand = '') {
      // Show indicator
      wakeIndicator.textContent = 'üéØ "Jarvis" detected!';
      wakeIndicator.classList.add('visible');
      clearTimeout(wakeWordTimeout);
      wakeWordTimeout = setTimeout(() => {
        wakeIndicator.classList.remove('visible');
      }, 2000);

      // Stop wake word listening
      stopWakeWordListening();

      // Start recording
      await startRecording();
      speechStartTime = Date.now();

      // Start silence detection for auto-stop
      startSilenceDetection();
    }

    // Fallback VAD for browsers without Speech Recognition
    function startVADListening() {
      stopVADListening();

      navigator.mediaDevices.getUserMedia({
        audio: { echoCancellation: true, noiseSuppression: true }
      }).then(stream => {
        vadStream = stream;
        vadContext = new AudioContext();
        const source = vadContext.createMediaStreamSource(stream);
        vadAnalyser = vadContext.createAnalyser();
        vadAnalyser.fftSize = 512;
        vadAnalyser.smoothingTimeConstant = 0.5;
        source.connect(vadAnalyser);

        voiceBtn.className = 'voice-button listening-passive';
        stateLabel.textContent = 'Listening... (speak to activate)';
        monitorAudioLevel();
      }).catch(err => {
        console.error('Failed to start VAD:', err);
        stateLabel.textContent = 'Microphone access denied';
        alwaysListenToggle.checked = false;
        alwaysListening = false;
        modeLabel.textContent = 'Push-to-talk';
      });
    }

    function stopVADListening() {
      if (vadContext) {
        vadContext.close();
        vadContext = null;
      }
      if (vadStream) {
        vadStream.getTracks().forEach(track => track.stop());
        vadStream = null;
      }
      vadAnalyser = null;
    }

    function monitorAudioLevel() {
      if (!vadAnalyser || !alwaysListening) return;

      const dataArray = new Uint8Array(vadAnalyser.frequencyBinCount);
      vadAnalyser.getByteFrequencyData(dataArray);
      const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;

      const now = Date.now();
      if (average > SILENCE_THRESHOLD && !isRecording && (now - lastTriggerTime) > TRIGGER_COOLDOWN) {
        lastTriggerTime = now;
        onVADSpeechDetected();
      }

      if (alwaysListening && !isRecording) {
        requestAnimationFrame(monitorAudioLevel);
      }
    }

    async function onVADSpeechDetected() {
      wakeIndicator.textContent = 'üéôÔ∏è Voice detected!';
      wakeIndicator.classList.add('visible');
      clearTimeout(wakeWordTimeout);
      wakeWordTimeout = setTimeout(() => {
        wakeIndicator.classList.remove('visible');
      }, 1500);

      stopVADListening();
      await startRecording();
      speechStartTime = Date.now();
      startSilenceDetection();
    }

    function startPassiveListening() {
      stopPassiveListening();
      startWakeWordListening();
    }

    function stopPassiveListening() {
      stopWakeWordListening();
      if (!isRecording) {
        voiceBtn.className = 'voice-button';
        stateLabel.textContent = 'Press to speak';
      }
    }

    function startSilenceDetection() {
      if (!recordingAnalyser || !isRecording || !recordingReady) return;

      let lastSpeechTime = Date.now();

      function checkSilence() {
        if (!isRecording || !alwaysListening || !recordingAnalyser || !recordingReady) return;

        const dataArray = new Uint8Array(recordingAnalyser.frequencyBinCount);
        recordingAnalyser.getByteFrequencyData(dataArray);
        const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;

        if (average > SILENCE_THRESHOLD) {
          lastSpeechTime = Date.now();
        }

        const silenceDuration = Date.now() - lastSpeechTime;
        const totalDuration = Date.now() - speechStartTime;

        // Stop if silence detected after minimum speech duration
        if (silenceDuration > SILENCE_DURATION && totalDuration > MIN_SPEECH_DURATION) {
          stopRecording();
          return;
        }

        // Max recording time of 30 seconds
        if (totalDuration > 30000) {
          stopRecording();
          return;
        }

        requestAnimationFrame(checkSilence);
      }

      // Start silence checking after a short delay
      setTimeout(checkSilence, 500);
    }

    // Toggle handler
    alwaysListenToggle.addEventListener('change', (e) => {
      alwaysListening = e.target.checked;
      modeLabel.textContent = alwaysListening ? 'Always listening' : 'Push-to-talk';

      if (alwaysListening) {
        startPassiveListening();
      } else {
        stopPassiveListening();
      }
    });

    // Load history and connect on page load
    loadHistory();
    connect();
  </script>
</body>
</html>
