<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Jarvis Voice Assistant</title>
  <style>
    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background: #0a0a0f;
      color: #e0e0e0;
      min-height: 100vh;
      display: flex;
      flex-direction: column;
      align-items: center;
      padding: 2rem;
    }
    h1 {
      color: #00d4ff;
      margin-bottom: 0.5rem;
    }
    .subtitle {
      color: #666;
      margin-bottom: 2rem;
    }
    .status {
      display: flex;
      align-items: center;
      gap: 0.5rem;
      margin-bottom: 1rem;
      padding: 0.5rem 1rem;
      background: #1a1a2e;
      border-radius: 20px;
    }
    .status-dot {
      width: 10px;
      height: 10px;
      border-radius: 50%;
      background: #ff4444;
    }
    .status-dot.connected {
      background: #44ff44;
    }
    .voice-button {
      width: 120px;
      height: 120px;
      border-radius: 50%;
      border: 3px solid #00d4ff;
      background: linear-gradient(145deg, #1a1a2e, #0a0a1a);
      color: #00d4ff;
      font-size: 2rem;
      cursor: pointer;
      transition: all 0.3s;
      display: flex;
      align-items: center;
      justify-content: center;
      margin: 2rem 0;
    }
    .voice-button:hover {
      transform: scale(1.05);
      box-shadow: 0 0 30px rgba(0, 212, 255, 0.3);
    }
    .voice-button.recording {
      background: linear-gradient(145deg, #2a1a2e, #1a0a1a);
      border-color: #ff4444;
      animation: pulse 1.5s infinite;
    }
    .voice-button.processing {
      border-color: #ffaa00;
    }
    .voice-button.speaking {
      border-color: #44ff44;
    }
    @keyframes pulse {
      0%, 100% { box-shadow: 0 0 20px rgba(255, 68, 68, 0.4); }
      50% { box-shadow: 0 0 40px rgba(255, 68, 68, 0.6); }
    }
    .state-label {
      font-size: 0.9rem;
      color: #888;
      margin-bottom: 2rem;
      text-transform: uppercase;
      letter-spacing: 2px;
    }
    .conversation {
      width: 100%;
      max-width: 600px;
      flex: 1;
      overflow-y: auto;
      padding: 1rem;
      background: #12121a;
      border-radius: 12px;
      margin-bottom: 1rem;
    }
    .message {
      padding: 1rem;
      margin-bottom: 1rem;
      border-radius: 12px;
      max-width: 85%;
    }
    .message.user {
      background: #1a2a3a;
      margin-left: auto;
      border-bottom-right-radius: 4px;
    }
    .message.assistant {
      background: #1a1a2e;
      border-bottom-left-radius: 4px;
    }
    .message .role {
      font-size: 0.75rem;
      color: #666;
      margin-bottom: 0.25rem;
      text-transform: uppercase;
    }
    .message.user .role {
      color: #00d4ff;
    }
    .message.assistant .role {
      color: #44ff44;
    }
    .interrupt-btn {
      position: fixed;
      bottom: 2rem;
      right: 2rem;
      padding: 1rem 2rem;
      background: #ff4444;
      color: white;
      border: none;
      border-radius: 8px;
      cursor: pointer;
      font-size: 1rem;
      display: none;
    }
    .interrupt-btn.visible {
      display: block;
    }
    .transcript {
      font-style: italic;
      color: #888;
      padding: 0.5rem;
      text-align: center;
    }
    .mode-toggle {
      display: flex;
      align-items: center;
      gap: 0.75rem;
      margin-bottom: 1rem;
      padding: 0.5rem 1rem;
      background: #1a1a2e;
      border-radius: 20px;
    }
    .toggle-switch {
      position: relative;
      width: 50px;
      height: 26px;
    }
    .toggle-switch input {
      opacity: 0;
      width: 0;
      height: 0;
    }
    .toggle-slider {
      position: absolute;
      cursor: pointer;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: #333;
      transition: 0.3s;
      border-radius: 26px;
    }
    .toggle-slider:before {
      position: absolute;
      content: "";
      height: 20px;
      width: 20px;
      left: 3px;
      bottom: 3px;
      background: white;
      transition: 0.3s;
      border-radius: 50%;
    }
    .toggle-switch input:checked + .toggle-slider {
      background: #00d4ff;
    }
    .toggle-switch input:checked + .toggle-slider:before {
      transform: translateX(24px);
    }
    .voice-button.listening-passive {
      border-color: #666;
      animation: pulse-passive 3s infinite;
    }
    @keyframes pulse-passive {
      0%, 100% { box-shadow: 0 0 10px rgba(102, 102, 102, 0.2); }
      50% { box-shadow: 0 0 20px rgba(102, 102, 102, 0.4); }
    }
    .wake-indicator {
      font-size: 0.8rem;
      color: #00d4ff;
      margin-top: -1rem;
      margin-bottom: 1rem;
      opacity: 0;
      transition: opacity 0.3s;
    }
    .wake-indicator.visible {
      opacity: 1;
    }
  </style>
</head>
<body>
  <h1>JARVIS</h1>
  <p class="subtitle">Real-time Voice Assistant</p>
  <p class="subtitle" style="font-size: 0.75rem; margin-top: -1rem;">Wake: "Jarvis" | Interrupt: "Stop", "Cancel", "Wait"</p>

  <div class="mode-toggle">
    <label class="toggle-switch">
      <input type="checkbox" id="alwaysListenToggle">
      <span class="toggle-slider"></span>
    </label>
    <span id="modeLabel">Push-to-talk</span>
  </div>

  <div class="status">
    <div class="status-dot" id="statusDot"></div>
    <span id="statusText">Disconnected</span>
  </div>

  <button class="voice-button" id="voiceBtn">üé§</button>
  <div class="wake-indicator" id="wakeIndicator">üéØ Wake word detected!</div>
  <div class="state-label" id="stateLabel">Press to speak</div>

  <div class="conversation" id="conversation"></div>

  <div class="transcript" id="transcript"></div>

  <button class="interrupt-btn" id="interruptBtn">‚èπ Interrupt</button>

  <script>
    const WS_URL = 'ws://localhost:3001';
    let ws = null;
    let mediaRecorder = null;
    let audioContext = null;
    let isRecording = false;
    let sessionId = null;
    let alwaysListening = false;
    let passiveStream = null;
    let passiveContext = null;
    let speechRecognition = null;
    let wakeWordTimeout = null;

    const voiceBtn = document.getElementById('voiceBtn');
    const stateLabel = document.getElementById('stateLabel');
    const statusDot = document.getElementById('statusDot');
    const statusText = document.getElementById('statusText');
    const conversation = document.getElementById('conversation');
    const transcript = document.getElementById('transcript');
    const interruptBtn = document.getElementById('interruptBtn');
    const alwaysListenToggle = document.getElementById('alwaysListenToggle');
    const modeLabel = document.getElementById('modeLabel');
    const wakeIndicator = document.getElementById('wakeIndicator');

    function connect() {
      ws = new WebSocket(WS_URL);

      ws.onopen = () => {
        statusDot.classList.add('connected');
        statusText.textContent = 'Connected';
        stateLabel.textContent = 'Press to speak';
      };

      ws.onclose = () => {
        statusDot.classList.remove('connected');
        statusText.textContent = 'Disconnected';
        stateLabel.textContent = 'Reconnecting...';
        setTimeout(connect, 3000);
      };

      ws.onerror = (error) => {
        console.error('WebSocket error:', error);
      };

      ws.onmessage = (event) => {
        const data = JSON.parse(event.data);
        handleMessage(data);
      };
    }

    function handleMessage(data) {
      console.log('Received:', data);

      switch (data.type) {
        case 'session.created':
          sessionId = data.sessionId;
          console.log('Session created:', sessionId);
          break;

        case 'transcript.partial':
          transcript.textContent = data.payload?.text || '';
          break;

        case 'transcript.final':
          transcript.textContent = '';
          addMessage('user', data.payload?.text || '');
          break;

        case 'llm.chunk':
          updateAssistantMessage(data.payload?.text || '');
          break;

        case 'llm.end':
          finalizeAssistantMessage();
          break;

        case 'tts.start':
          voiceBtn.className = 'voice-button speaking';
          stateLabel.textContent = 'Speaking...';
          interruptBtn.classList.add('visible');
          break;

        case 'tts.chunk':
          // Play audio chunk
          if (data.payload?.audio) {
            playAudio(data.payload.audio);
          }
          break;

        case 'tts.end':
          interruptBtn.classList.remove('visible');
          if (alwaysListening) {
            startPassiveListening();
          } else {
            voiceBtn.className = 'voice-button';
            stateLabel.textContent = 'Press to speak';
          }
          break;

        case 'session.interrupt':
          interruptBtn.classList.remove('visible');
          if (alwaysListening) {
            startPassiveListening();
          } else {
            voiceBtn.className = 'voice-button';
            stateLabel.textContent = 'Interrupted - Press to speak';
          }
          break;

        case 'session.state_change':
          console.log('State changed:', data.payload);
          break;

        case 'error':
          console.error('Error:', data.payload);
          stateLabel.textContent = 'Error: ' + (data.payload?.message || 'Unknown');
          break;
      }
    }

    let currentAssistantMessage = '';
    let assistantMessageEl = null;

    function addMessage(role, content) {
      const div = document.createElement('div');
      div.className = `message ${role}`;
      div.innerHTML = `<div class="role">${role}</div><div class="content">${content}</div>`;
      conversation.appendChild(div);
      conversation.scrollTop = conversation.scrollHeight;
      return div;
    }

    function updateAssistantMessage(chunk) {
      if (!assistantMessageEl) {
        assistantMessageEl = addMessage('assistant', '');
        currentAssistantMessage = '';
      }
      currentAssistantMessage += chunk;
      assistantMessageEl.querySelector('.content').textContent = currentAssistantMessage;
      conversation.scrollTop = conversation.scrollHeight;
    }

    function finalizeAssistantMessage() {
      assistantMessageEl = null;
      currentAssistantMessage = '';
    }

    let recordingAnalyser = null;
    let recordingStream = null;
    let recordingReady = false;
    let pendingStop = false;

    async function startRecording() {
      // Set isRecording immediately to prevent race conditions
      isRecording = true;
      recordingReady = false;
      pendingStop = false;
      voiceBtn.className = 'voice-button recording';
      stateLabel.textContent = 'Starting...';

      try {
        recordingStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            sampleRate: 16000,
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true
          }
        });

        // Check if stop was requested while waiting for mic permission
        if (pendingStop) {
          cleanupRecording();
          return;
        }

        audioContext = new AudioContext({ sampleRate: 16000 });
        const source = audioContext.createMediaStreamSource(recordingStream);
        const processor = audioContext.createScriptProcessor(4096, 1, 1);

        // Create analyser for silence detection
        recordingAnalyser = audioContext.createAnalyser();
        recordingAnalyser.fftSize = 512;
        recordingAnalyser.smoothingTimeConstant = 0.5;
        source.connect(recordingAnalyser);

        processor.onaudioprocess = (e) => {
          if (!isRecording || !recordingReady) return;

          const inputData = e.inputBuffer.getChannelData(0);
          const pcmData = new Int16Array(inputData.length);

          for (let i = 0; i < inputData.length; i++) {
            pcmData[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768));
          }

          if (ws && ws.readyState === WebSocket.OPEN) {
            ws.send(pcmData.buffer);
          }
        };

        source.connect(processor);
        processor.connect(audioContext.destination);

        recordingReady = true;
        stateLabel.textContent = 'Listening...';

        // Check again if stop was requested while setting up
        if (pendingStop) {
          stopRecording();
        }

      } catch (err) {
        console.error('Error accessing microphone:', err);
        isRecording = false;
        recordingReady = false;
        voiceBtn.className = 'voice-button';
        stateLabel.textContent = 'Microphone access denied';
      }
    }

    function cleanupRecording() {
      isRecording = false;
      recordingReady = false;
      recordingAnalyser = null;

      if (recordingStream) {
        recordingStream.getTracks().forEach(track => track.stop());
        recordingStream = null;
      }

      if (audioContext) {
        audioContext.close();
        audioContext = null;
      }

      voiceBtn.className = 'voice-button';
      stateLabel.textContent = 'Press to speak';
    }

    function stopRecording() {
      // If recording not ready yet, set flag to stop when ready
      if (!recordingReady && isRecording) {
        pendingStop = true;
        return;
      }

      const hadAudio = recordingReady;
      cleanupRecording();

      // Only show processing and send audio.end if we actually recorded something
      if (hadAudio) {
        voiceBtn.className = 'voice-button processing';
        stateLabel.textContent = 'Processing...';

        if (ws && ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify({ type: 'audio.end' }));
        }
      }

      // Don't restart passive listening here - wait for tts.end event
    }

    function playAudio(base64Audio) {
      // TODO: Implement audio playback from base64/buffer
      console.log('Audio chunk received');
    }

    voiceBtn.addEventListener('mousedown', () => {
      if (!isRecording && ws && ws.readyState === WebSocket.OPEN) {
        startRecording();
      }
    });

    voiceBtn.addEventListener('mouseup', () => {
      if (isRecording) {
        stopRecording();
      }
    });

    voiceBtn.addEventListener('mouseleave', () => {
      if (isRecording) {
        stopRecording();
      }
    });

    interruptBtn.addEventListener('click', () => {
      if (ws && ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify({ type: 'interrupt' }));
      }
    });

    // Keyboard shortcut: Space to talk
    document.addEventListener('keydown', (e) => {
      if (e.code === 'Space' && !e.repeat && !isRecording) {
        e.preventDefault();
        if (ws && ws.readyState === WebSocket.OPEN) {
          startRecording();
        }
      }
    });

    document.addEventListener('keyup', (e) => {
      if (e.code === 'Space' && isRecording) {
        e.preventDefault();
        stopRecording();
      }
    });

    // Voice Activity Detection (VAD) - works offline
    let vadContext = null;
    let vadStream = null;
    let vadAnalyser = null;
    let silenceTimeout = null;
    let speechStartTime = null;
    let lastTriggerTime = 0;
    const SILENCE_THRESHOLD = 30; // Audio level threshold (higher = less sensitive)
    const SILENCE_DURATION = 1500; // ms of silence before stopping
    const MIN_SPEECH_DURATION = 500; // ms minimum to count as speech
    const TRIGGER_COOLDOWN = 3000; // ms cooldown between triggers

    function startPassiveListening() {
      // Stop any existing VAD
      stopPassiveListening();

      navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true
        }
      }).then(stream => {
        vadStream = stream;
        vadContext = new AudioContext();
        const source = vadContext.createMediaStreamSource(stream);
        vadAnalyser = vadContext.createAnalyser();
        vadAnalyser.fftSize = 512;
        vadAnalyser.smoothingTimeConstant = 0.5;
        source.connect(vadAnalyser);

        voiceBtn.className = 'voice-button listening-passive';
        stateLabel.textContent = 'Listening... (speak to activate)';

        // Start monitoring audio levels
        monitorAudioLevel();
      }).catch(err => {
        console.error('Failed to start VAD:', err);
        stateLabel.textContent = 'Microphone access denied';
        alwaysListenToggle.checked = false;
        alwaysListening = false;
        modeLabel.textContent = 'Push-to-talk';
      });
    }

    function monitorAudioLevel() {
      if (!vadAnalyser || !alwaysListening) return;

      const dataArray = new Uint8Array(vadAnalyser.frequencyBinCount);
      vadAnalyser.getByteFrequencyData(dataArray);

      // Calculate average audio level
      const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;

      const now = Date.now();
      if (average > SILENCE_THRESHOLD && !isRecording && (now - lastTriggerTime) > TRIGGER_COOLDOWN) {
        // Speech detected - start recording
        lastTriggerTime = now;
        onSpeechDetected();
      }

      // Continue monitoring if still in always-listening mode and not recording
      if (alwaysListening && !isRecording) {
        requestAnimationFrame(monitorAudioLevel);
      }
    }

    async function onSpeechDetected() {
      // Show indicator
      wakeIndicator.textContent = 'üéôÔ∏è Voice detected!';
      wakeIndicator.classList.add('visible');
      clearTimeout(wakeWordTimeout);
      wakeWordTimeout = setTimeout(() => {
        wakeIndicator.classList.remove('visible');
        wakeIndicator.textContent = 'üéØ Wake word detected!';
      }, 1500);

      // Stop VAD temporarily
      if (vadContext) {
        vadContext.close();
        vadContext = null;
      }
      if (vadStream) {
        vadStream.getTracks().forEach(track => track.stop());
        vadStream = null;
      }

      // Start active recording and wait for it to be ready
      await startRecording();
      speechStartTime = Date.now();

      // Start silence detection for auto-stop (now recordingAnalyser is ready)
      startSilenceDetection();
    }

    function startSilenceDetection() {
      if (!recordingAnalyser || !isRecording || !recordingReady) return;

      let lastSpeechTime = Date.now();

      function checkSilence() {
        if (!isRecording || !alwaysListening || !recordingAnalyser || !recordingReady) return;

        const dataArray = new Uint8Array(recordingAnalyser.frequencyBinCount);
        recordingAnalyser.getByteFrequencyData(dataArray);
        const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;

        if (average > SILENCE_THRESHOLD) {
          lastSpeechTime = Date.now();
        }

        const silenceDuration = Date.now() - lastSpeechTime;
        const totalDuration = Date.now() - speechStartTime;

        // Stop if silence detected after minimum speech duration
        if (silenceDuration > SILENCE_DURATION && totalDuration > MIN_SPEECH_DURATION) {
          stopRecording();
          return;
        }

        // Max recording time of 30 seconds
        if (totalDuration > 30000) {
          stopRecording();
          return;
        }

        requestAnimationFrame(checkSilence);
      }

      // Start silence checking after a short delay
      setTimeout(checkSilence, 500);
    }

    function stopPassiveListening() {
      if (vadContext) {
        vadContext.close();
        vadContext = null;
      }
      if (vadStream) {
        vadStream.getTracks().forEach(track => track.stop());
        vadStream = null;
      }
      vadAnalyser = null;

      if (!isRecording) {
        voiceBtn.className = 'voice-button';
        stateLabel.textContent = 'Press to speak';
      }
    }

    // Toggle handler
    alwaysListenToggle.addEventListener('change', (e) => {
      alwaysListening = e.target.checked;
      modeLabel.textContent = alwaysListening ? 'Always listening' : 'Push-to-talk';

      if (alwaysListening) {
        startPassiveListening();
      } else {
        stopPassiveListening();
      }
    });

    // Connect on load
    connect();
  </script>
</body>
</html>
